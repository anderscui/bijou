{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 豆瓣电影短评分类 - 使用 LSTM\n",
    "\n",
    "## 数据集\n",
    "\n",
    "Dataset: https://www.kaggle.com/utmhikari/doubanmovieshortcomments，共包含212万条评论数据，这里只考虑两个分类，从1星和5星评论中各取10万条。本文的实现主要参考了 [Long Short-Term Memory](https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/)。\n",
    "\n",
    "# 如何使用 LSTM\n",
    "\n",
    "关于 LSTM 的工作机制，可以查看[李沐的书](https://zh-v2.d2l.ai/chapter_recurrent-modern/lstm.html)。关于 LSTM 在 PyTorch 中的实现细节，可以查看其[官方文档](https://pytorch.org/docs/1.9.1/generated/torch.nn.LSTM.html)。这里尝试对使用 LSTM 处理文本做一个直观的理解。\n",
    "\n",
    "## Tokenize\n",
    "\n",
    "LSTM 适用于时间序列问题，文本是天然的时间序列。不过这个序列到底是什么并不那么显然。对于英语、法语这样的语言，可以考虑使用\"词\"，对于汉语，则可以考虑使用\"字\"或\"词\"。使用字或词是较为传统的方法，Bert、RoBERTa 等基于 Transformer 架构的模型则使用不同的 Subword 算法。上述这些方法做的事情，都属于 tokenizer，它将文本 tokenize 为一个 token 的序列。本文处理的是汉语的情形，使用最简单的\"字\"序列。\n",
    "\n",
    "有了 token 序列，接下来需要将这些 token 转换为数值表示，最简单的处理方式是收集数据集中的所有 token，为每个 token 分配一个数值 id，这种方式本质上得到的是 one-hot 编码。\n",
    "\n",
    "## Embedding 层\n",
    "\n",
    "one-hot 编码是极其稀疏的，而且 token 的表示与其语义毫无关联，因此通常处理方法是将 one-hot 转换为 word embedding，这正是 `torch.nnEmbedding` 做的事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding lookup table: torch.Size([100, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[-0.2362, -0.7930, -0.3985,  0.2873,  0.9603],\n        [ 1.8757,  0.9257, -0.9709, -0.3830, -0.3318],\n        [ 1.4579,  2.0634, -0.2326, -0.7954,  0.0490]])"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = 100\n",
    "embedding_dim = 5\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "# embedding 是一个 lookup table，它将 `vocab_size` 个 token 分别表示为 `embedding_dim` 大小的向量\n",
    "print(f'embedding lookup table:', embedding.weight.data.shape)\n",
    "\n",
    "# 如果一句话由三个 token 构成，其 id 分别是 [2, 1, 3]\n",
    "sent_token_ids = [2, 1, 3]\n",
    "embedded = embedding(torch.LongTensor(sent_token_ids))\n",
    "\n",
    "# 这句话被表示为 (3, 5) 大小的矩阵\n",
    "embedded.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这样，一个文本序列表示为了矩阵，但每个文本长短不一，因此需要统一其长度，过长的截断，过短的则要填充。到这里就得到了可以输入到 LSTM 层的数据。\n",
    "\n",
    "## LSTM 层\n",
    "\n",
    "通过下面的代码来查看 LSTM 输入和输出的情况。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([3, 10, 5])\n",
      "last_layer_outputs shape:  torch.Size([3, 10, 6])\n",
      "last_hidden_states shape:  torch.Size([2, 3, 6])\n",
      "last_cell_states shape:  torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "# 每次输入的形状：(batch_size, seq_len, embedding_dim)\n",
    "# 即 `batch_size` 个文本，每个文本长度 `seq_len`，每个 token 表示为 `embedding_dim` 大小的向量\n",
    "batch_size = 3\n",
    "seq_len = 10\n",
    "embedding_dim = 5\n",
    "\n",
    "# LSTM 的参数\n",
    "n_layers = 2\n",
    "hidden_dim = 6\n",
    "lstm_layer = nn.LSTM(num_layers=n_layers,\n",
    "                     input_size=embedding_dim,\n",
    "                     hidden_size=hidden_dim,\n",
    "                     batch_first=True)\n",
    "\n",
    "# 这里测试一下输入和输出的样子\n",
    "inputs = torch.randn(batch_size, seq_len, embedding_dim)\n",
    "print(f'input shape: {inputs.shape}')\n",
    "\n",
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "hidden = (hidden_state, cell_state)\n",
    "\n",
    "last_layer_outs, (last_hidden_states, last_cell_states) = lstm_layer(inputs, hidden)\n",
    "print(f'last_layer_outputs shape: ', last_layer_outs.shape)\n",
    "print(f'last_hidden_states shape: ', last_hidden_states.shape)\n",
    "print(f'last_cell_states shape: ', last_cell_states.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 分类任务\n",
    "\n",
    "也就是说，LSTM 层的输出是各种隐藏状态，接下来需要通过这些隐藏状态实现分类。下面使用的是 LSTM 层返回值的第一部分，即最后一层的所有隐藏状态，将其连接到一个 `nn.Linear` 层，再辅以 `nn.Sigmoid`，因为这里要实现的是二分类。\n",
    "\n",
    "至此对数据在整个流程中的变化有所了解了，下面是具体的实现。\n",
    "\n",
    "# 代码实现\n",
    "\n",
    "## 加载数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(200000,\n 1    100000\n 5    100000\n Name: Star, dtype: int64)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('douban_movie_comments.csv', usecols=['Comment', 'Star'])\n",
    "len(df), df.Star.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(200000, 200000)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    return text.strip().lower()\n",
    "\n",
    "comments = [preprocess_text(c) for c in df.Comment]\n",
    "labels = [1 if star == 5 else 0 for star in df.Star]\n",
    "\n",
    "len(comments), len(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(140000, 30000, 30000, 30000)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_comments, val_test_comments, train_labels, val_test_labels = train_test_split(comments, labels, test_size=0.3, shuffle=False)\n",
    "val_comments, test_comments, val_labels, test_labels = train_test_split(val_test_comments,\n",
    "                                                                        val_test_labels,\n",
    "                                                                        test_size=0.5,\n",
    "                                                                        shuffle=False)\n",
    "\n",
    "len(train_comments), len(val_comments), len(test_comments), len(val_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 构建词汇表"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "PADDING = '[PAD]'\n",
    "UNKNOWN = '[UNK]'\n",
    "\n",
    "\n",
    "def build_vocab(texts: List[str], tokenizer, min_token_freq=1):\n",
    "    c_tokens = Counter()\n",
    "    for text in texts:\n",
    "        for token in tokenizer(text):\n",
    "            c_tokens[token] += 1\n",
    "\n",
    "    if min_token_freq > 1:\n",
    "        c_tokens = Counter({k: v for k, v in c_tokens.items() if v >= MIN_FREQ})\n",
    "\n",
    "    vocab = [PADDING, UNKNOWN] + [w for w, _ in c_tokens.most_common()]\n",
    "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "    idx2word = {i: w for w, i in word2idx.items()}\n",
    "    return word2idx, idx2word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(5313, 5313)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIN_FREQ = 2\n",
    "tokenizer=lambda text: list(text)\n",
    "\n",
    "word2idx, idx2word = build_vocab(train_comments,\n",
    "                                 tokenizer=tokenizer,\n",
    "                                 min_token_freq=MIN_FREQ)\n",
    "\n",
    "len(word2idx), len(idx2word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 文本长度统计"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大句子长度：196, 最小句子长度：1, 平均句子长度：33.95, 句子长度中位数：20.00\n",
      "            text_len\n",
      "count  140000.000000\n",
      "mean       33.946979\n",
      "std        35.106944\n",
      "min         1.000000\n",
      "25%        10.000000\n",
      "50%        20.000000\n",
      "75%        45.000000\n",
      "max       196.000000\n"
     ]
    }
   ],
   "source": [
    "text_lengths = [len(s) for s in train_comments]\n",
    "print('最大句子长度：{}, 最小句子长度：{}, 平均句子长度：{:.2f}, 句子长度中位数：{:.2f}'.format(\n",
    "    max(text_lengths), min(text_lengths), np.mean(text_lengths), np.median(text_lengths)))\n",
    "\n",
    "df_len = pd.DataFrame({'text_len': text_lengths})\n",
    "print(df_len.describe())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_len.plot.hist('text_len',  grid=True, title='text length', bins=50);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "绝大多数句子程度都在 100 以下，因此序列长度设置为 100。\n",
    "\n",
    "## 文本的向量化"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[853,\n 123,\n 39,\n 1189,\n 185,\n 41,\n 617,\n 514,\n 424,\n 173,\n 774,\n 1043,\n 2,\n 27,\n 13,\n 39,\n 23,\n 44,\n 94,\n 8,\n 29,\n 147,\n 64,\n 12,\n 94,\n 170,\n 29,\n 2,\n 52,\n 20,\n 47,\n 84,\n 26,\n 94,\n 238,\n 29,\n 170,\n 29,\n 12,\n 21,\n 94,\n 8,\n 29,\n 7,\n 4]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_to_vec(sent: str):\n",
    "    sent_word_ids = []\n",
    "    for word in tokenizer(sent):\n",
    "        sent_word_ids.append(word2idx.get(word) or word2idx[UNKNOWN])\n",
    "    return sent_word_ids\n",
    "\n",
    "train_vectors = [text_to_vec(sent) for sent in train_comments]\n",
    "val_vectors = [text_to_vec(sent) for sent in val_comments]\n",
    "test_vectors = [text_to_vec(sent) for sent in test_comments]\n",
    "\n",
    "# every sentence is a [word_id]\n",
    "train_vectors[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "((100,),\n array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n         853,  123,   39, 1189,  185,   41,  617,  514,  424,  173,  774,\n        1043,    2,   27,   13,   39,   23,   44,   94,    8,   29,  147,\n          64,   12,   94,  170,   29,    2,   52,   20,   47,   84,   26,\n          94,  238,   29,  170,   29,   12,   21,   94,    8,   29,    7,\n           4]))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad texts\n",
    "def pad_input(sent_vecs, seq_len):\n",
    "    features = np.full((len(sent_vecs), seq_len), fill_value=word2idx[PADDING], dtype=int)\n",
    "    for i, sent_vec in enumerate(sent_vecs):\n",
    "        if len(sent_vec) != 0:\n",
    "            features[i, -len(sent_vec):] = np.array(sent_vec)[:seq_len]\n",
    "    return features\n",
    "\n",
    "\n",
    "seq_len = 100\n",
    "train_vectors = pad_input(train_vectors, seq_len)\n",
    "val_vectors = pad_input(val_vectors, seq_len)\n",
    "test_vectors = pad_input(test_vectors, seq_len)\n",
    "\n",
    "train_vectors[0].shape, train_vectors[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(140000, 30000, 30000)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "val_labels = np.array(val_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "len(train_labels), len(val_labels), len(test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 准备模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_vectors), torch.from_numpy(train_labels))\n",
    "val_data = TensorDataset(torch.from_numpy(val_vectors), torch.from_numpy(val_labels))\n",
    "test_data = TensorDataset(torch.from_numpy(test_vectors), torch.from_numpy(test_labels))\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check GPUs\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# define layer\n",
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(num_layers=n_layers, input_size=embedding_dim, hidden_size=hidden_dim,\n",
    "                            dropout=drop_prob,\n",
    "                            batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        #print(f'x size: {x.shape}')\n",
    "        batch_size = x.size(0)\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "\n",
    "        # lstm_out shape: (batch_size, seq_len, hidden_size)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        out = self.dropout(lstm_out)\n",
    "        # out shape -> (batch_size * seq_len, 1)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        # out shape -> (batch_size, seq_len)\n",
    "        out = out.view(batch_size, -1)\n",
    "        # out shape -> (batch_size,)\n",
    "        out = out[:, -1]\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "SentimentNet(\n  (embedding): Embedding(5313, 50)\n  (lstm): LSTM(50, 32, num_layers=2, batch_first=True, dropout=0.5)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=32, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(word2idx)\n",
    "output_size = 1\n",
    "embedding_dim = 50\n",
    "hidden_dim = 32\n",
    "n_layers = 2\n",
    "\n",
    "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "\n",
    "lr = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3... Step: 100... Loss: 0.405695... Val Loss: 0.400934 Time elapsed: 23.160\n",
      "Validation loss decreased (inf --> 0.400934).  Saving model ...\n",
      "Epoch: 1/3... Step: 200... Loss: 0.408278... Val Loss: 0.351721 Time elapsed: 46.190\n",
      "Validation loss decreased (0.400934 --> 0.351721).  Saving model ...\n",
      "Epoch: 1/3... Step: 300... Loss: 0.419433... Val Loss: 0.315268 Time elapsed: 69.263\n",
      "Validation loss decreased (0.351721 --> 0.315268).  Saving model ...\n",
      "Epoch: 1/3... Step: 400... Loss: 0.339693... Val Loss: 0.294527 Time elapsed: 92.483\n",
      "Validation loss decreased (0.315268 --> 0.294527).  Saving model ...\n",
      "Epoch: 1/3... Step: 500... Loss: 0.255131... Val Loss: 0.284580 Time elapsed: 115.489\n",
      "Validation loss decreased (0.294527 --> 0.284580).  Saving model ...\n",
      "Epoch: 2/3... Step: 600... Loss: 0.340236... Val Loss: 0.281590 Time elapsed: 138.568\n",
      "Validation loss decreased (0.284580 --> 0.281590).  Saving model ...\n",
      "Epoch: 2/3... Step: 700... Loss: 0.294741... Val Loss: 0.290420 Time elapsed: 161.933\n",
      "Epoch: 2/3... Step: 800... Loss: 0.204488... Val Loss: 0.277600 Time elapsed: 184.935\n",
      "Validation loss decreased (0.281590 --> 0.277600).  Saving model ...\n",
      "Epoch: 2/3... Step: 900... Loss: 0.265462... Val Loss: 0.270833 Time elapsed: 207.921\n",
      "Validation loss decreased (0.277600 --> 0.270833).  Saving model ...\n",
      "Epoch: 2/3... Step: 1000... Loss: 0.217337... Val Loss: 0.266721 Time elapsed: 231.032\n",
      "Validation loss decreased (0.270833 --> 0.266721).  Saving model ...\n",
      "Epoch: 3/3... Step: 1100... Loss: 0.251273... Val Loss: 0.263820 Time elapsed: 254.530\n",
      "Validation loss decreased (0.266721 --> 0.263820).  Saving model ...\n",
      "Epoch: 3/3... Step: 1200... Loss: 0.271364... Val Loss: 0.266794 Time elapsed: 277.541\n",
      "Epoch: 3/3... Step: 1300... Loss: 0.272219... Val Loss: 0.266256 Time elapsed: 300.730\n",
      "Epoch: 3/3... Step: 1400... Loss: 0.295436... Val Loss: 0.258578 Time elapsed: 323.751\n",
      "Validation loss decreased (0.263820 --> 0.258578).  Saving model ...\n",
      "Epoch: 3/3... Step: 1500... Loss: 0.258546... Val Loss: 0.267271 Time elapsed: 346.952\n",
      "Epoch: 3/3... Step: 1600... Loss: 0.287733... Val Loss: 0.258747 Time elapsed: 369.864\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 3\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "start = time.time()\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        if counter % print_every == 0:\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
    "                  \"Time elapsed: {:.3f}\".format(time.time() - start))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 模型加载"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.254\n",
      "Test accuracy: 89.533%\n"
     ]
    }
   ],
   "source": [
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('./state_dict.pt'))\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "h = model.init_hidden(batch_size)\n",
    "\n",
    "model.eval()\n",
    "for inputs, labels in test_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    output, h = model(inputs, h)\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}%\".format(test_acc * 100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 预测"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def predict_labels(sents: List[str]):\n",
    "    test_sent_words = [tokenizer(sent) for sent in sents]\n",
    "    test_vecs = [[word2idx.get(w) or word2idx.get(UNKNOWN) for w in words] for words in test_sent_words]\n",
    "    test_vecs = pad_input(test_vecs, seq_len)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_h = model.init_hidden(len(test_sents))\n",
    "    test_output, test_h = model(torch.from_numpy(test_vecs), test_h)\n",
    "    test_pred = torch.round(test_output.squeeze())\n",
    "\n",
    "    predicted = []\n",
    "    for pred, prob in zip(test_pred, test_output.squeeze()):\n",
    "        predicted.append({'label': pred.item(), 'prob': prob.item()})\n",
    "    return predicted"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.005 史上最烂的电影，没有之一\n",
      "1 0.997 我最喜欢的电影，太棒了\n",
      "0 0.012 不打一星对不起我的电影票钱\n",
      "0 0.008 无语\n",
      "0 0.010 无力吐槽\n",
      "0 0.231 不是我喜欢的类型\n",
      "1 0.968 要推荐给所有朋友\n",
      "0 0.229 不会推荐给任何朋友\n",
      "1 0.935 居然看哭了！\n",
      "0 0.039 我想我以后不会再看这个导演的作品了\n"
     ]
    }
   ],
   "source": [
    "test_sents = ['史上最烂的电影，没有之一',\n",
    "              '我最喜欢的电影，太棒了',\n",
    "              '不打一星对不起我的电影票钱',\n",
    "              '无语',\n",
    "              '无力吐槽',\n",
    "              '不是我喜欢的类型',\n",
    "              '要推荐给所有朋友',\n",
    "              '不会推荐给任何朋友',\n",
    "              '居然看哭了！',\n",
    "              '我想我以后不会再看这个导演的作品了']\n",
    "\n",
    "predicted = predict_labels(test_sents)\n",
    "for pred, sent in zip(predicted, test_sents):\n",
    "    print(int(pred['label']), f'{pred[\"prob\"]:.3f}', sent)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}